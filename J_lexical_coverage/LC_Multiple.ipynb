{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries \n",
    "##### and also the cefr-j wordlist (list of tuples) and lists of special tokens from .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from spacy.attrs import ORTH\n",
    "import spacy\n",
    "from cj_tuples import cj_wordlist\n",
    "from special_tokens import compounds, hyphens, plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add special cases to the tokenizer\n",
    "##### to tokenize compounds, hyphenated words and plural countable compound nouns (from the compounds and hyphenated words)\n",
    "##### words with periods and most of the apostrophe words (except driver's license) were tokenized by spaCy anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_case_1 = [{ORTH: \"driver's license\"}]\n",
    "nlp.tokenizer.add_special_case(\"driver's license\", special_case_1)\n",
    "\n",
    "for compound in compounds:\n",
    "    special_case_2 = [{ORTH: compound}]\n",
    "    nlp.tokenizer.add_special_case(compound, special_case_2)\n",
    "\n",
    "for hyphen in hyphens:\n",
    "    special_case_3 = [{ORTH: hyphen}]\n",
    "    nlp.tokenizer.add_special_case(hyphen, special_case_3)\n",
    "\n",
    "for plural in plurals:\n",
    "    special_case_4 = [{ORTH: plural}]\n",
    "    nlp.tokenizer.add_special_case(plural, special_case_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the POS tag for compound words\n",
    "##### without this several of the compound words were given the wrong tag (e.g. PROPN)\n",
    "##### so I specified that all occurences of a special token should have the POS tag specified in the CEFR-j list\n",
    "##### there are 7 tokens that have 2 POS tags 'all right', 'upside down', 'full-time', 'grown-up', 'half-price', 'part-time', 'second-hand'\n",
    "##### sometimes they will be given the wrong tag, but it won't affect the lexical coverage, as these tokens' POS tags are both at the same CEFR level \n",
    "##### (e.g. ('all right', 'ADJ', 'a1') ('all right', 'ADV', 'a1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "# this updates the POS of all compounds to that in the cefr-j list\n",
    "# the tag is still the original spacy tag for the compound\n",
    "# this is OK for me because I am only working with POS\n",
    "# and it has the benefit of retaining information about which token's POS have been amended\n",
    "for compound in compounds:\n",
    "    for item in cj_wordlist:\n",
    "        if compound == item[0]:\n",
    "            compound_patterns = [[{\"LOWER\": compound}]]\n",
    "            compound_attrs = {\"POS\": item[1]}\n",
    "            ruler.add(patterns=compound_patterns,attrs=compound_attrs)\n",
    "\n",
    "for hyphen in hyphens:\n",
    "    for item in cj_wordlist:\n",
    "        if hyphen == item[0]:\n",
    "            hyphen_patterns = [[{\"LOWER\": hyphen}]]\n",
    "            hyphen_attrs = {\"POS\": item[1]}\n",
    "            ruler.add(patterns=hyphen_patterns,attrs=hyphen_attrs)\n",
    "            \n",
    "for plural in plurals:\n",
    "    plural_patterns = [[{\"LOWER\": plural}]]\n",
    "    plural_attrs = {\"POS\": \"NOUN\"}\n",
    "    ruler.add(patterns=plural_patterns,attrs=plural_attrs)\n",
    "    \n",
    "number_patterns = [[{\"LIKE_NUM\": True}]]\n",
    "number_attrs = {\"POS\": \"NUM\"}\n",
    "ruler.add(patterns=number_patterns,attrs=number_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the channel for the transcripts you want to assess the lexical coverage of\n",
    "##### it's better to do it in batches like this, because the processing time is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_in_df = 'WION (IN)'\n",
    "channel_for_output = 'WION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7566    let's begin with turkey where another economic...\n",
       "7567    and here's the story for science buffs the hea...\n",
       "7568    our next story is from iran which continues to...\n",
       "7569    let's shift our attention over to south korea ...\n",
       "7570    ramayana and mahabharata two of the greatest e...\n",
       "                              ...                        \n",
       "8281    u.s authorities arrested the wife of jailed me...\n",
       "8282    and countries across the globe have been react...\n",
       "8283    archaeologists have hailed a latest discovery ...\n",
       "8284    and the u.s troops withdrawal from afghanistan...\n",
       "8285    and in a bid to accelerate the vaccination rol...\n",
       "Name: text, Length: 720, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('world_news_720_df.csv')\n",
    "df = df[df['channel'] == channel_in_df] \n",
    "# df = df[0:6]\n",
    "texts = df['text']\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the spaCy pipeline to the text (tokenizer, POS tagger, lemmatizer...)\n",
    "##### also create a no punctuation version of the doc after applying the pipeline\n",
    "##### this is to calculate the lexical coverage (to avoid counting punctuation as tokens)\n",
    "##### and print the tokens with POS tags and lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign the tokens to lists\n",
    "##### all_level_dicts is for an open list of dictionaries\n",
    "##### for each text, create a dictionary with an open list for each level (propn_num, A1, A2...)\n",
    "##### then append each dictionary to all_level_dicts\n",
    "##### proper nouns and numbers are added to the propn_num list first\n",
    "##### Check if the token and pos match a tuple in the cefr-j list of tuples, and if they do, add it to the appropriate cefr level list\n",
    "##### If none of the conditions are met, append the 'others' list\n",
    "##### including 'break' was necessary to append others to the others list with the else statement\n",
    "### Also added in extra lists for pos tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_level_dicts = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    level_dict = {'Text': [], 'doc_length': [], 'PROPN': [], 'NUM': [], 'A1': [], 'A1_pos': [], 'A2': [], 'A2_pos': [], \n",
    "                  'B1': [], 'B1_pos': [], 'B2': [], 'B2_pos': [], 'others': [], 'others_pos': []}\n",
    "    level_dict['Text'].append(i)\n",
    "    doc = nlp(text)\n",
    "    no_punc_doc = [token for token in doc if not token.is_punct]\n",
    "    level_dict['doc_length'].append(len(no_punc_doc))\n",
    "\n",
    "    for token in no_punc_doc:\n",
    "        for item in cj_wordlist:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                level_dict['PROPN'].append(token.lemma_)\n",
    "                break\n",
    "            elif token.pos_ == \"NUM\":\n",
    "                level_dict['NUM'].append(token.lemma_)\n",
    "                break\n",
    "            elif token.lemma_ == item[0] and token.pos_ == item[1] and item[2] == 'a1':\n",
    "                level_dict['A1'].append(token.lemma_)\n",
    "                level_dict['A1_pos'].append(token.pos_)\n",
    "                break\n",
    "            elif token.lemma_ == item[0] and token.pos_ == item[1] and item[2] == 'a2':\n",
    "                level_dict['A2'].append(token.lemma_)\n",
    "                level_dict['A2_pos'].append(token.pos_)\n",
    "                break\n",
    "            elif token.lemma_ == item[0] and token.pos_ == item[1] and item[2] == 'b1':\n",
    "                level_dict['B1'].append(token.lemma_)\n",
    "                level_dict['B1_pos'].append(token.pos_)\n",
    "                break\n",
    "            elif token.lemma_ == item[0] and token.pos_ == item[1] and item[2] == 'b2':\n",
    "                level_dict['B2'].append(token.lemma_)\n",
    "                level_dict['B2_pos'].append(token.pos_)\n",
    "                break\n",
    "        else:\n",
    "            level_dict['others'].append(token.lemma_)\n",
    "            level_dict['others_pos'].append(token.pos_)\n",
    "\n",
    "            \n",
    "    all_level_dicts.append(level_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe with the channel name, video_id, doc_length, lists of PROPN and NUM\n",
    "#### Then lists of tuples (word, POS) for A1-B2 words and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = pd.DataFrame(all_level_dicts)\n",
    "dict_df.insert(1, 'channel', channel_in_df)\n",
    "# add the video_id column from the original filtered df (from this channel)\n",
    "dict_df.insert(2, 'video_id', df['video_id'])\n",
    "dict_df2 = dict_df.drop(columns=['Text'])\n",
    "\n",
    "# https://stackoverflow.com/questions/56714623/zip-list-elements-in-different-dataframe-columns\n",
    "\n",
    "dict_df2['A1+pos'] = dict_df2.apply(lambda x: list(zip(x.A1, x.A1_pos)), axis=1)\n",
    "dict_df2['A2+pos'] = dict_df2.apply(lambda x: list(zip(x.A2, x.A2_pos)), axis=1)\n",
    "dict_df2['B1+pos'] = dict_df2.apply(lambda x: list(zip(x.B1, x.B1_pos)), axis=1)\n",
    "dict_df2['B2+pos'] = dict_df2.apply(lambda x: list(zip(x.B2, x.B2_pos)), axis=1)\n",
    "dict_df2['others+pos'] = dict_df2.apply(lambda x: list(zip(x.others, x.others_pos)), axis=1)\n",
    "\n",
    "dict_df3 = dict_df2.drop(columns=['A1', 'A1_pos', 'A2', 'A2_pos', 'B1', 'B1_pos', 'B2', 'B2_pos', 'others', 'others_pos'])\n",
    "\n",
    "dict_df3\n",
    "\n",
    "dict_df3.to_csv('LC_Output/words/'+channel_for_output+'_cefr_words.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the cumulative lexical coverage\n",
    "##### convert the results to a pandas dataframe and display it\n",
    "##### output the file to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [item['doc_length'] for item in all_level_dicts]\n",
    "doc_length_flat = [item for sublist in doc_lengths for item in sublist]\n",
    "\n",
    "propn_freq = []\n",
    "propn_lists = [item['PROPN'] for item in all_level_dicts]\n",
    "propn_freq.append([len(item) for item in propn_lists])\n",
    "propn_flat_freq = [item for sublist in propn_freq for item in sublist]\n",
    "propn_decimal = [int(freq) / int(length) for freq,length in zip(propn_flat_freq, doc_length_flat)]\n",
    "propn_percent = [i * 100 for i in propn_decimal]\n",
    "\n",
    "num_freq = []\n",
    "num_lists = [item['NUM'] for item in all_level_dicts]\n",
    "num_freq.append([len(item) for item in num_lists])\n",
    "num_flat_freq = [item for sublist in num_freq for item in sublist]\n",
    "num_decimal = [int(freq) / int(length) for freq,length in zip(num_flat_freq, doc_length_flat)]\n",
    "num_percent = [i * 100 for i in num_decimal]\n",
    "\n",
    "a1_freq = []\n",
    "a1_lists = [item['A1'] for item in all_level_dicts]\n",
    "a1_freq.append([len(item) for item in a1_lists])\n",
    "a1_flat_freq = [item for sublist in a1_freq for item in sublist]\n",
    "a1_decimal = [int(freq) / int(length) for freq,length in zip(a1_flat_freq, doc_length_flat)]\n",
    "a1_percent = [i * 100 for i in a1_decimal]\n",
    "\n",
    "a2_freq = []\n",
    "a2_lists = [item['A2'] for item in all_level_dicts]\n",
    "a2_freq.append([len(item) for item in a2_lists])\n",
    "a2_flat_freq = [item for sublist in a2_freq for item in sublist]\n",
    "a2_decimal = [int(freq) / int(length) for freq,length in zip(a2_flat_freq, doc_length_flat)]\n",
    "a2_percent = [i * 100 for i in a2_decimal]\n",
    "\n",
    "b1_freq = []\n",
    "b1_lists = [item['B1'] for item in all_level_dicts]\n",
    "b1_freq.append([len(item) for item in b1_lists])\n",
    "b1_flat_freq = [item for sublist in b1_freq for item in sublist]\n",
    "b1_decimal = [int(freq) / int(length) for freq,length in zip(b1_flat_freq, doc_length_flat)]\n",
    "b1_percent = [i * 100 for i in b1_decimal]\n",
    "\n",
    "b2_freq = []\n",
    "b2_lists = [item['B2'] for item in all_level_dicts]\n",
    "b2_freq.append([len(item) for item in b2_lists])\n",
    "b2_flat_freq = [item for sublist in b2_freq for item in sublist]\n",
    "b2_decimal = [int(freq) / int(length) for freq,length in zip(b2_flat_freq, doc_length_flat)]\n",
    "b2_percent = [i * 100 for i in b2_decimal]\n",
    "\n",
    "others_freq = []\n",
    "others_lists = [item['others'] for item in all_level_dicts]\n",
    "others_freq.append([len(item) for item in others_lists])\n",
    "others_flat_freq = [item for sublist in others_freq for item in sublist]\n",
    "others_decimal = [int(freq) / int(length) for freq,length in zip(others_flat_freq, doc_length_flat)]\n",
    "others_percent = [i * 100 for i in others_decimal]\n",
    "\n",
    "data = {'propn%': propn_percent, 'num%': num_percent, 'A1%': a1_percent, 'A2%': a2_percent, \n",
    "        'B1%': b1_percent, 'B2%': b2_percent, 'others%': others_percent}\n",
    "\n",
    "percent_df = pd.DataFrame(data)\n",
    "\n",
    "percent_df['channel'] = channel_in_df\n",
    "# add the video_id column from the original filtered df (from this channel)\n",
    "percent_df['video_id'] = df['video_id']\n",
    "percent_df['propn%_cum'] = percent_df['propn%']\n",
    "percent_df['num%_cum'] = percent_df['propn%_cum'] + percent_df['num%']\n",
    "percent_df['A1%_cum'] = percent_df['num%_cum'] + percent_df['A1%']\n",
    "percent_df['A2%_cum'] = percent_df['A1%_cum'] + percent_df['A2%']\n",
    "percent_df['B1%_cum'] = percent_df['A2%_cum'] + percent_df['B1%']\n",
    "percent_df['B2%_cum'] = percent_df['B1%_cum'] + percent_df['B2%']\n",
    "percent_df['others%_cum'] = percent_df['B2%_cum'] + percent_df['others%']\n",
    "\n",
    "percent_df2 = percent_df.drop(columns=['propn%', 'num%', 'A1%', 'A2%','B1%', 'B2%', 'others%'])\n",
    "\n",
    "percent_df2\n",
    "\n",
    "percent_df2.to_csv('LC_Output/'+channel_for_output+'_cefr_LC.csv', encoding='utf8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
