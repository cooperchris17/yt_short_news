{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import cefr-j wordlist (the format is a list of tuples)\n",
    "##### and pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cj_tuples import cj_wordlist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Adjectives (in 3 steps)\n",
    "#### Read in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>per_million</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>doc_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>more</td>\n",
       "      <td>7299</td>\n",
       "      <td>2558.110356</td>\n",
       "      <td>4060</td>\n",
       "      <td>48.998310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new</td>\n",
       "      <td>4935</td>\n",
       "      <td>1729.589616</td>\n",
       "      <td>2736</td>\n",
       "      <td>33.019551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>4020</td>\n",
       "      <td>1408.905827</td>\n",
       "      <td>2792</td>\n",
       "      <td>33.695390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many</td>\n",
       "      <td>3677</td>\n",
       "      <td>1288.693215</td>\n",
       "      <td>2426</td>\n",
       "      <td>29.278301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first</td>\n",
       "      <td>3560</td>\n",
       "      <td>1247.687747</td>\n",
       "      <td>2429</td>\n",
       "      <td>29.314506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7321</th>\n",
       "      <td>abortive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>ir</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>yaer</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>botanical</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>lithic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.350474</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7326 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency  per_million  doc_freq  doc_percentage\n",
       "0          more       7299  2558.110356      4060       48.998310\n",
       "1           new       4935  1729.589616      2736       33.019551\n",
       "2         other       4020  1408.905827      2792       33.695390\n",
       "3          many       3677  1288.693215      2426       29.278301\n",
       "4         first       3560  1247.687747      2429       29.314506\n",
       "...         ...        ...          ...       ...             ...\n",
       "7321   abortive          1     0.350474         1        0.012069\n",
       "7322         ir          1     0.350474         1        0.012069\n",
       "7323       yaer          1     0.350474         1        0.012069\n",
       "7324  botanical          1     0.350474         1        0.012069\n",
       "7325     lithic          1     0.350474         1        0.012069\n",
       "\n",
       "[7326 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('World_adj_frequency.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a list of adjectives at the A1 and A2 level in the CEFR-J wordlist\n",
    "#### item[0] = the word, item[1] = the pos, item[2] = the CEFR level (in the list of tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OK',\n",
       " 'ok',\n",
       " 'Olympic',\n",
       " 'olympic',\n",
       " 'acceptable',\n",
       " 'additional',\n",
       " 'adult',\n",
       " 'advanced',\n",
       " 'afraid',\n",
       " 'aged',\n",
       " 'alive',\n",
       " 'all right',\n",
       " 'alone',\n",
       " 'alright',\n",
       " 'amused',\n",
       " 'ancient',\n",
       " 'angry',\n",
       " 'annoying',\n",
       " 'anxious',\n",
       " 'appropriate',\n",
       " 'armed',\n",
       " 'artificial',\n",
       " 'asleep',\n",
       " 'assistant',\n",
       " 'attractive',\n",
       " 'audio',\n",
       " 'automatic',\n",
       " 'average',\n",
       " 'awake',\n",
       " 'awful',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'basic',\n",
       " 'beautiful',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'black',\n",
       " 'blank',\n",
       " 'blonde',\n",
       " 'blue',\n",
       " 'boiled',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'brave',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'broken',\n",
       " 'brown',\n",
       " 'busy',\n",
       " 'careful',\n",
       " 'certain',\n",
       " 'cheap',\n",
       " 'chemical',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'clever',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'cloudy',\n",
       " 'cold',\n",
       " 'colorful',\n",
       " 'colourful',\n",
       " 'comfortable',\n",
       " 'comic',\n",
       " 'common',\n",
       " 'complete',\n",
       " 'confident',\n",
       " 'confused',\n",
       " 'convenient',\n",
       " 'cool',\n",
       " 'correct',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'creative',\n",
       " 'crowded',\n",
       " 'cute',\n",
       " 'daily',\n",
       " 'dangerous',\n",
       " 'dark',\n",
       " 'dead',\n",
       " 'dear',\n",
       " 'delicious',\n",
       " 'developed',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'direct',\n",
       " 'dirty',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'dishonest',\n",
       " 'dizzy',\n",
       " 'double',\n",
       " 'dressed',\n",
       " 'dry',\n",
       " 'due',\n",
       " 'early',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'educational',\n",
       " 'elderly',\n",
       " 'electric',\n",
       " 'elementary',\n",
       " 'embarrassing',\n",
       " 'empty',\n",
       " 'endangered',\n",
       " 'energetic',\n",
       " 'enormous',\n",
       " 'everyday',\n",
       " 'excellent',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'expensive',\n",
       " 'extra',\n",
       " 'fair',\n",
       " 'false',\n",
       " 'familiar',\n",
       " 'famous',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'fascinating',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'female',\n",
       " 'few',\n",
       " 'final',\n",
       " 'fine',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'foggy',\n",
       " 'following',\n",
       " 'foreign',\n",
       " 'frank',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'fried',\n",
       " 'frightened',\n",
       " 'frightening',\n",
       " 'front',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'further',\n",
       " 'gentle',\n",
       " 'given',\n",
       " 'glad',\n",
       " 'gold',\n",
       " 'golden',\n",
       " 'good',\n",
       " 'good-looking',\n",
       " 'grateful',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'greedy',\n",
       " 'green',\n",
       " 'grey',\n",
       " 'grilled',\n",
       " 'half-price',\n",
       " 'handicapped',\n",
       " 'handsome',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'harmful',\n",
       " 'healthy',\n",
       " 'heavy',\n",
       " 'helpful',\n",
       " 'high',\n",
       " 'hot',\n",
       " 'human',\n",
       " 'hungry',\n",
       " 'ideal',\n",
       " 'ill',\n",
       " 'illegal',\n",
       " 'impatient',\n",
       " 'important',\n",
       " 'impossible',\n",
       " 'indoor',\n",
       " 'inexpensive',\n",
       " 'inner',\n",
       " 'instant',\n",
       " 'intelligent',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'international',\n",
       " 'junior',\n",
       " 'key',\n",
       " 'kind',\n",
       " 'kindly',\n",
       " 'large',\n",
       " 'last',\n",
       " 'late',\n",
       " 'latest',\n",
       " 'lazy',\n",
       " 'left',\n",
       " 'left-hand',\n",
       " 'light',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'lively',\n",
       " 'living',\n",
       " 'local',\n",
       " 'logical',\n",
       " 'lonely',\n",
       " 'long',\n",
       " 'loose',\n",
       " 'lost',\n",
       " 'lovely',\n",
       " 'low',\n",
       " 'lucky',\n",
       " 'lyric',\n",
       " 'mad',\n",
       " 'magic',\n",
       " 'major',\n",
       " 'many',\n",
       " 'married',\n",
       " 'marvellous',\n",
       " 'marvelous',\n",
       " 'mean',\n",
       " 'medical',\n",
       " 'merry',\n",
       " 'middle',\n",
       " 'military',\n",
       " 'missing',\n",
       " 'mobile',\n",
       " 'modern',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'musical',\n",
       " 'mysterious',\n",
       " 'national',\n",
       " 'native',\n",
       " 'natural',\n",
       " 'necessary',\n",
       " 'negative',\n",
       " 'nervous',\n",
       " 'net',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'noisy',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'off',\n",
       " 'official',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'online',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opposite',\n",
       " 'orange',\n",
       " 'organised',\n",
       " 'organized',\n",
       " 'original',\n",
       " 'other',\n",
       " 'overweight',\n",
       " 'own',\n",
       " 'pacific',\n",
       " 'peaceful',\n",
       " 'perfect',\n",
       " 'personal',\n",
       " 'physical',\n",
       " 'pink',\n",
       " 'plastic',\n",
       " 'playful',\n",
       " 'pleasant',\n",
       " 'pleased',\n",
       " 'pleasing',\n",
       " 'polite',\n",
       " 'political',\n",
       " 'poor',\n",
       " 'popular',\n",
       " 'possible',\n",
       " 'powerful',\n",
       " 'precise',\n",
       " 'pretty',\n",
       " 'principal',\n",
       " 'private',\n",
       " 'professional',\n",
       " 'progressive',\n",
       " 'proper',\n",
       " 'purple',\n",
       " 'quick',\n",
       " 'quiet',\n",
       " 'rainy',\n",
       " 'raw',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'recent',\n",
       " 'red',\n",
       " 'regular',\n",
       " 'relaxed',\n",
       " 'remote',\n",
       " 'retired',\n",
       " 'rich',\n",
       " 'right',\n",
       " 'right-hand',\n",
       " 'romantic',\n",
       " 'royal',\n",
       " 'rude',\n",
       " 'sad',\n",
       " 'safe',\n",
       " 'same',\n",
       " 'sandy',\n",
       " 'scientific',\n",
       " 'second',\n",
       " 'senior',\n",
       " 'separate',\n",
       " 'several',\n",
       " 'short',\n",
       " 'shy',\n",
       " 'sick',\n",
       " 'significant',\n",
       " 'silly',\n",
       " 'silver',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'single',\n",
       " 'singular',\n",
       " 'sleepless',\n",
       " 'sleepy',\n",
       " 'slim',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smart',\n",
       " 'smooth',\n",
       " 'snowy',\n",
       " 'social',\n",
       " 'soft',\n",
       " 'sorry',\n",
       " 'south',\n",
       " 'special',\n",
       " 'specific',\n",
       " 'split',\n",
       " 'square',\n",
       " 'still',\n",
       " 'straight',\n",
       " 'strange',\n",
       " 'strict',\n",
       " 'strong',\n",
       " 'successful',\n",
       " 'sudden',\n",
       " 'suitable',\n",
       " 'sunny',\n",
       " 'super',\n",
       " 'sure',\n",
       " 'surprised',\n",
       " 'surprising',\n",
       " 'sweet',\n",
       " 'tall',\n",
       " 'teenage',\n",
       " 'terrible',\n",
       " 'terrorist',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'third',\n",
       " 'thirsty',\n",
       " 'tidy',\n",
       " 'tight',\n",
       " 'tired',\n",
       " 'traditional',\n",
       " 'true',\n",
       " 'ugly',\n",
       " 'uncertain',\n",
       " 'uncomfortable',\n",
       " 'underwater',\n",
       " 'uneasy',\n",
       " 'unfair',\n",
       " 'unforgettable',\n",
       " 'unhappy',\n",
       " 'unhealthy',\n",
       " 'uniform',\n",
       " 'unimportant',\n",
       " 'unknown',\n",
       " 'unlike',\n",
       " 'unnecessary',\n",
       " 'unpleasant',\n",
       " 'unusual',\n",
       " 'upset',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'usual',\n",
       " 'violent',\n",
       " 'warm',\n",
       " 'weak',\n",
       " 'weekly',\n",
       " 'well',\n",
       " 'well-known',\n",
       " 'west',\n",
       " 'wet',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wide',\n",
       " 'wild',\n",
       " 'windy',\n",
       " 'wise',\n",
       " 'wonderful',\n",
       " 'wooden',\n",
       " 'worried',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'wrong',\n",
       " 'yellow',\n",
       " 'young']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_a2_adj = []\n",
    "for item in cj_wordlist:\n",
    "    if item[1] == 'ADJ' and item[2] == 'a1':\n",
    "        a1_a2_adj.append(item[0])\n",
    "    if item[1] == 'ADJ' and item[2] == 'a2':\n",
    "        a1_a2_adj.append(item[0])\n",
    "        \n",
    "a1_a2_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe, deleting any of the A1 and A2 level adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in a1_a2_adj:  \n",
    "    df = df[df['word'].str.contains(word)==False]\n",
    "    \n",
    "df.to_csv('World_adj_B1_plus.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow the same process for other parts of speech\n",
    "##### Nouns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('World_noun_frequency.csv')\n",
    "\n",
    "a1_a2_noun = []\n",
    "for item in cj_wordlist:\n",
    "    if item[1] == 'NOUN' and item[2] == 'a1':\n",
    "        a1_a2_noun.append(item[0])\n",
    "    if item[1] == 'NOUN' and item[2] == 'a2':\n",
    "        a1_a2_noun.append(item[0])\n",
    "\n",
    "# filter out words that are in the a1_a2 list\n",
    "for word in a1_a2_noun:\n",
    "    df = df[df['word'].str.contains(word)==False]\n",
    "    # for nouns one 'word' was a hyphen, this is to filter out the hyphen\n",
    "    df = df[df['word'].str.isalpha()]\n",
    "\n",
    "# filter out 'one' and 'other'\n",
    "# after manually reviewing the list, they might be difficult to identify as nouns by learners using the ShinyConc concordancer\n",
    "df = df[df['word'].str.contains('one|other')==False]\n",
    "\n",
    "df.to_csv('World_noun_B1_plus.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('World_verb_frequency.csv')\n",
    "\n",
    "a1_a2_verb = []\n",
    "for item in cj_wordlist:\n",
    "    if item[1] == 'VERB' and item[2] == 'a1':\n",
    "        a1_a2_verb.append(item[0])\n",
    "    if item[1] == 'VERB' and item[2] == 'a2':\n",
    "        a1_a2_verb.append(item[0])\n",
    "\n",
    "# filter out words that are in the a1_a2 list\n",
    "for word in a1_a2_verb:\n",
    "    df = df[df['word'].str.contains(word)==False]\n",
    "    # for verbs one 'word' was a hyphen, this is to filter out the hyphen\n",
    "    df = df[df['word'].str.isalpha()]\n",
    "\n",
    "# filter 'accord' out (after manually reviewing the list), accord is the lemma of 'according' \n",
    "# mainly 'according to', which isn't a verb, I didn't tokenize any compounds (like I did for lexical coverage) for the frequency lists\n",
    "# so according was incorrectly tagged as a verb, when 'according to' should be a preposition (ADP)\n",
    "df = df[df['word'].str.contains('accord')==False]\n",
    "    \n",
    "df.to_csv('World_verb_B1_plus.csv', encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
